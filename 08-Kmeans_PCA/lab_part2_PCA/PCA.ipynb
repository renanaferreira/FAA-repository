{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Principal Component Analysis (PCA)\n",
    "\n",
    "In this exercise, you will use principal component analysis (PCA) to perform\n",
    "dimensionality reduction. You will ﬁrst experiment with a simple 2D\n",
    "dataset, and then use it on a bigger dataset of 5000 face image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Data\n",
    "\n",
    "To help you understand how PCA works, you will ﬁrst start with a 2D dataset\n",
    "which has one direction of large variation and one of smaller variation. \n",
    "You will visualize what happens when you use PCA to reduce th data from 2D to 1D. \n",
    "\n",
    "Load matrix X from matlab file \"ex7data1.mat\" and assign it to X3. \n",
    "\n",
    "Print the shape of X3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X3 = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "\n",
    "\n",
    "Plot the data and get a figure similar to Fig.1. \n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : **2D data** </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing PCA\n",
    "\n",
    "\n",
    "Before using PCA, it is important to ﬁrst normalize the data by subtracting the mean value of each feature from the dataset, and scaling each dimension so that they are in the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    \"\"\"\n",
    "    Returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1.\n",
    "    \"\"\"\n",
    "    mu = np.mean(X,axis=0)\n",
    "    sigma = np.std(X,axis=0)\n",
    "    \n",
    "    X_norm = (X - mu)/sigma\n",
    "    \n",
    "    return X_norm, mu , sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA consists of two steps: First, compute the covariance matrix S of the data:\n",
    "\n",
    "$S = \\frac{1}{m} X^TX$\n",
    "\n",
    "Then, use SVD function to compute the eigenvectors U(1), U(2), ...U(n), that are the principal components of variation in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    \"\"\"\n",
    "    Computes eigenvectors of the covariance matrix of X\n",
    "    \"\"\"\n",
    "    m,n = X.shape[0], X.shape[1]\n",
    "    \n",
    "    sigma = 1/m * X.T @ X\n",
    "    # U contain the principal components;\n",
    "    #S contain a diagonal matrix of singular values\n",
    "    \n",
    "    U,S,V = svd(sigma)\n",
    "    \n",
    "    return U,S,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply feature normalization and pca for X3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "X_norm,mu,std =  ?\n",
    "U,S = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing PCA\n",
    "\n",
    " Plot the corresponding principal components found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X3[:,0],X3[:,1],marker=\"o\",facecolors=\"none\",edgecolors=\"b\")\n",
    "plt.plot([mu[0],(mu+S[0]*U[:,0].T)[0]],[mu[1],(mu+S[0]*U[:,0].T)[1]],color=\"black\",linewidth=3)\n",
    "plt.plot([mu[0],(mu+S[1]*U[:,1].T)[0]],[mu[1],(mu+S[1]*U[:,1].T)[1]],color=\"black\",linewidth=3)\n",
    "plt.xlim(-1,7)\n",
    "plt.ylim(2,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the top principal component (eigenvector) found\n",
    "#you should expect to see an output of about [-0.707 -0.707]. \n",
    "\n",
    "print ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with PCA\n",
    "\n",
    "After computing the principal components, you can use them to reduce the\n",
    "feature dimension of your dataset by projecting each example onto a lower\n",
    "dimensional space, x (i) → z (i) (e.g., projecting the data from 2D to 1D). \n",
    "\n",
    "Now you will use the eigenvectors returned by PCA and project the 2D dataset into a 1D space.\n",
    "\n",
    "In practice, if you were using a learning algorithm such as linear regression\n",
    "or perhaps neural networks, you could now use the projected data instead\n",
    "of the original data. By using the projected data, you can train your model\n",
    "faster as there are less dimensions in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectData(X, U, K):\n",
    "    \"\"\"\n",
    "    Computes the reduced data representation when projecting only on to the top k eigenvectors\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    U_reduced = U[:,:K]\n",
    "    Z = np.zeros((m,K))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(K):\n",
    "            Z[i,j] = X[i,:] @ U_reduced[:,j]\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data onto K=1 dimension\n",
    "K=1\n",
    "Z = projectData(X_norm, U, K)\n",
    "print(\"Projection of the first example:\",Z[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing an approximation of the data\n",
    "\n",
    "After projecting the data onto the lower dimensional space (Z), you can approximately recover the data (X_rec) by projecting them back onto the original high dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoverData(Z, U, K):\n",
    "    \"\"\"\n",
    "    Recovers an approximation of the original data when using the projected data\n",
    "    \"\"\"\n",
    "    m,n = Z.shape[0],U.shape[0]\n",
    "    X_rec = np.zeros((m,n))\n",
    "    U_reduced = U[:,:K]\n",
    "    \n",
    "    for i in range(m):\n",
    "        X_rec[i,:] = Z[i,:] @ U_reduced.T\n",
    "    \n",
    "    return X_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rec  = ?\n",
    "print(\"Approximation of the first example:\",X_rec[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the projections\n",
    "\n",
    "The projection retains only the information in the direction given by U(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_norm[:,0],X_norm[:,1],marker=\"o\",label=\"Original\",facecolors=\"none\",edgecolors=\"b\",s=15)\n",
    "plt.scatter(X_rec[:,0],X_rec[:,1],marker=\"o\",label=\"Approximation\",facecolors=\"none\",edgecolors=\"r\",s=15)\n",
    "plt.title(\"The Normalized and Projected Data after PCA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Image Dataset\n",
    "\n",
    "In this part of the exercise, you will run PCA on face images to see how it can be used in practice for dimension reduction. File ex7faces.mat, contains a dataset X of face images, each 32 × 32 in grayscale. Each row of X corresponds to one face image (a row vector of length 1024). \n",
    "\n",
    "\n",
    "Load matrix X from matlab file \"ex7data1.mat\" and assign it to X4. \n",
    "Print the shape of X4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 =? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the dataset\n",
    "\n",
    "Load and visualize the ﬁrst 100 of these face images (Fig.2). \n",
    "\n",
    "<img src=\"images/f3.png\" style=\"width:450px;height:450px;\">\n",
    "<caption><center> **Fig. 2** : **Face data** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=10,ncols=10,figsize=(8,8))\n",
    "for i in range(0,100,10):\n",
    "    for j in range(10):\n",
    "        ax[int(i/10),j].imshow(X4[i+j,:].reshape(32,32,order=\"F\"),cmap=\"gray\")\n",
    "        ax[int(i/10),j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on face\n",
    "\n",
    "To run PCA on the face dataset, you need ﬁrst to normalize the dataset as before.  \n",
    "\n",
    "After running PCA, you will obtain the principal components of the dataset. Each principal component in U (each row) is a vector of length n (where for the face dataset,\n",
    "n = 1024). We can visualize these principal components by reshaping each of them into a 32 × 32 matrix that corresponds to the pixels in the original dataset. \n",
    "The code below displays the ﬁrst 36 principal components that describe the largest variations. \n",
    "If you want, you can also change the code to display more principal components to see how\n",
    "they capture more and more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm2 = ?\n",
    "\n",
    "# Run PCA\n",
    "U2 = ?\n",
    "\n",
    "#Visualize the top 36 eigenvectors found\n",
    "U_reduced = U2[:,:36].T\n",
    "fig2, ax2 = plt.subplots(6,6,figsize=(8,8))\n",
    "for i in range(0,36,6):\n",
    "    for j in range(6):\n",
    "        ax2[int(i/6),j].imshow(U_reduced[i+j,:].reshape(32,32,order=\"F\"),cmap=\"gray\")\n",
    "        ax2[int(i/6),j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "You can use the computed principal components, to reduce the dimension of the face dataset. \n",
    "\n",
    "Project the data into smaller dimension matrix  Z2 (e.g., 100 dimensions). \n",
    "Print the dimension of Z2. \n",
    "\n",
    "This allows you to use your learning algorithm with a smaller input size instead of the original 1024 dimensions. This can help speed up your learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K2 = ?\n",
    "Z2 = ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the data and visualize the reconstructed data to get something similar to Fig. 3. \n",
    "\n",
    "From the reconstruction, you can observe that the general structure and appearance of the face are kept while the ﬁne details are lost. This is a remarkable reduction (more than 10×) in the dataset size that can help speed up your learning algorithm signiﬁcantly.\n",
    "For example, if you were training a neural network to perform person recognition (gven a face image, predict the identitfy of the person), you can use the dimension reduced input of only a 100 dimensions instead of the original pixels.\n",
    "\n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:450px;height:450px;\">\n",
    "<caption><center> **Fig. 3** : **Training data** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reconstruction\n",
    "X_rec2  = ?\n",
    "\n",
    "# Visualize the reconstructed data\n",
    "fig3, ax3 = plt.subplots(10,10,figsize=(8,8))\n",
    "for i in range(0,100,10):\n",
    "    for j in range(10):\n",
    "        ax3[int(i/10),j].imshow(X_rec2[i+j,:].reshape(32,32,order=\"F\"),cmap=\"gray\")\n",
    "        ax3[int(i/10),j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
